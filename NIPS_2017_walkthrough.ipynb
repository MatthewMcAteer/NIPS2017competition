{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalised Medicine - EDA with tidy R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Clinically Actionable Genetic Mutations: NIPS 2017 Competition\n",
    "\n",
    "A lot has been said during the past several years about how precision medicine and, more concretely, how genetic testing is going to disrupt the way diseases like cancer are treated.\n",
    "\n",
    "But this is only partially happening due to the huge amount of manual work still required. Memorial Sloan Kettering Cancer Center (MSKCC) recently launched this competition on Kaggle, accepted by the NIPS 2017 Competition Track, because they need the help of data scientists to take personalized medicine to its full potential.\n",
    "\n",
    "Once sequenced, a cancer tumor can have thousands of genetic mutations. But the challenge is distinguishing the mutations that contribute to tumor growth (drivers) from the neutral mutations (passengers). \n",
    "\n",
    "Currently this interpretation of genetic mutations is being done manually. This is a very time-consuming task where a clinical pathologist has to manually review and classify every single genetic mutation based on evidence from text-based clinical literature.\n",
    "\n",
    "For this competition MSKCC is making available an expert-annotated knowledge base where world-class researchers and oncologists have manually annotated thousands of mutations.\n",
    "\n",
    "We need your help to develop a Machine Learning algorithm that, using this knowledge base as a baseline, automatically classifies genetic variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 million models submitted to competitions last year, and they're on track to pass 2 million within the year. That's alot of machinelearning. The CEO of Kaggle recently gave a talk to ORielly MediaPretty well positioned to see  about the patterns that Kaggle has been observing across the competitions.\n",
    "\n",
    "There are wto approaches that typically win on competitions. The first of this is on Structured Data problems. This is the type of data that typically fits in an SQL database or an Excel spreadsheet. If you want to win a competitions\n",
    "1. Explore the data. \n",
    "Explore the data every which way you can. What does each variable mean? What is it's distribution? Ideally, how was it collected? Once you have a rock-solid understanding of the data, the second thing you want to do is...\n",
    "2. Create and select features\n",
    "Predicting which cars would be good buys at a second-hand car competion. One of the winners found out that car-color had a big effect. The winner didn't just specify color, but he grouped the cars into standard-color cars and unusual-colored cars, on the assumption that an unusual-colored car would have been better taken care of, because it was owned by an enthusiast. And then finally...\n",
    "3. Parameter tuning and ensembling\n",
    "Then random-forest really toook of in the early days. Kaggle has since moved beyond random forrest, and the new top performing algorithm is called gradient boosting machines, which is dominating the structured data problems. The implementation is called xgboost. There's another one out of microsoft research that is doing very well for by GBM. Now the choice of classifier, and how one tunes the classifier, actually matters a lot less than the choosing of the featrures. Many people think that do do machine learning you need to be able to do a lot of math, but it turns out the real magic is in choosing the features, which is a lot less challenging than designing complex algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this competiton, there are a few pre-designated files.\n",
    "\n",
    "* **training_variants** - a comma separated file containing the description of the genetic mutations used for training. Fields are ID (the id of the row used to link the mutation to the clinical evidence), Gene (the gene where this genetic mutation is located), Variation (the aminoacid change for this mutations), Class (1-9 the class this genetic mutation has been classified on)\n",
    "* **training_text** - a double pipe (||) delimited file that contains the clinical evidence (text) used to classify genetic mutations. Fields are ID (the id of the row used to link the clinical evidence to the genetic mutation), Text (the clinical evidence used to classify the genetic mutation)\n",
    "* **test_variants** - a comma separated file containing the description of the genetic mutations used for training. Fields are ID (the id of the row used to link the mutation to the clinical evidence), Gene (the gene where this genetic mutation is located), Variation (the aminoacid change for this mutations)\n",
    "* **test_text** - a double pipe (||) delimited file that contains the clinical evidence (text) used to classify genetic mutations. Fields are ID (the id of the row used to link the clinical evidence to the genetic mutation), Text (the clinical evidence used to classify the genetic mutation)\n",
    "* **submissionSample** - a sample submission file in the correct format\n",
    "\n",
    "Let's start with loading these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import *\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "train = pd.read_csv('../input/training_variants')\n",
    "test = pd.read_csv('../input/test_variants')\n",
    "trainx = pd.read_csv('../input/training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "testx = pd.read_csv('../input/test_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "\n",
    "train = pd.merge(train, trainx, how='left', on='ID').fillna('')\n",
    "y = train['Class'].values\n",
    "train = train.drop(['Class'], axis=1)\n",
    "\n",
    "test = pd.merge(test, testx, how='left', on='ID').fillna('')\n",
    "pid = test['ID'].values\n",
    "\n",
    "df_all = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "df_all['Gene_Share'] = df_all.apply(lambda r: sum([1 for w in r['Gene'].split(' ') if w in r['Text'].split(' ')]), axis=1)\n",
    "df_all['Variation_Share'] = df_all.apply(lambda r: sum([1 for w in r['Variation'].split(' ') if w in r['Text'].split(' ')]), axis=1)\n",
    "\n",
    "#commented for Kaggle Limits\n",
    "#for i in range(56):\n",
    "#    df_all['Gene_'+str(i)] = df_all['Gene'].map(lambda x: str(x[i]) if len(x)>i else '')\n",
    "#    df_all['Variation'+str(i)] = df_all['Variation'].map(lambda x: str(x[i]) if len(x)>i else '')\n",
    "\n",
    "\n",
    "gen_var_lst = sorted(list(train.Gene.unique()) + list(train.Variation.unique()))\n",
    "print(len(gen_var_lst))\n",
    "gen_var_lst = [x for x in gen_var_lst if len(x.split(' '))==1]\n",
    "print(len(gen_var_lst))\n",
    "i_ = 0\n",
    "#commented for Kaggle Limits\n",
    "#for gen_var_lst_itm in gen_var_lst:\n",
    "#    if i_ % 100 == 0: print(i_)\n",
    "#    df_all['GV_'+str(gen_var_lst_itm)] = df_all['Text'].map(lambda x: str(x).count(str(gen_var_lst_itm)))\n",
    "#    i_ += 1\n",
    "\n",
    "for c in df_all.columns:\n",
    "    if df_all[c].dtype == 'object':\n",
    "        if c in ['Gene','Variation']:\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            df_all[c+'_lbl_enc'] = lbl.fit_transform(df_all[c].values)  \n",
    "            df_all[c+'_len'] = df_all[c].map(lambda x: len(str(x)))\n",
    "            df_all[c+'_words'] = df_all[c].map(lambda x: len(str(x).split(' ')))\n",
    "        elif c != 'Text':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            df_all[c] = lbl.fit_transform(df_all[c].values)\n",
    "        if c=='Text': \n",
    "            df_all[c+'_len'] = df_all[c].map(lambda x: len(str(x)))\n",
    "            df_all[c+'_words'] = df_all[c].map(lambda x: len(str(x).split(' '))) \n",
    "\n",
    "train = df_all.iloc[:len(train)]\n",
    "test = df_all.iloc[len(train):]\n",
    "\n",
    "class cust_regression_vals(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, x):\n",
    "        x = x.drop(['Gene', 'Variation','ID','Text'],axis=1).values\n",
    "        return x\n",
    "\n",
    "class cust_txt_col(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, x):\n",
    "        return x[self.key].apply(str)\n",
    "\n",
    "print('Pipeline...')\n",
    "fp = pipeline.Pipeline([\n",
    "    ('union', pipeline.FeatureUnion(\n",
    "        n_jobs = -1,\n",
    "        transformer_list = [\n",
    "            ('standard', cust_regression_vals()),\n",
    "            ('pi1', pipeline.Pipeline([('Gene', cust_txt_col('Gene')), ('count_Gene', feature_extraction.text.CountVectorizer(analyzer=u'char', ngram_range=(1, 8))), ('tsvd1', decomposition.TruncatedSVD(n_components=20, n_iter=25, random_state=12))])),\n",
    "            ('pi2', pipeline.Pipeline([('Variation', cust_txt_col('Variation')), ('count_Variation', feature_extraction.text.CountVectorizer(analyzer=u'char', ngram_range=(1, 8))), ('tsvd2', decomposition.TruncatedSVD(n_components=20, n_iter=25, random_state=12))])),\n",
    "            #commented for Kaggle Limits\n",
    "            #('pi3', pipeline.Pipeline([('Text', cust_txt_col('Text')), ('tfidf_Text', feature_extraction.text.TfidfVectorizer(ngram_range=(1, 2))), ('tsvd3', decomposition.TruncatedSVD(n_components=50, n_iter=25, random_state=12))]))\n",
    "        ])\n",
    "    )])\n",
    "\n",
    "train = fp.fit_transform(train); print(train.shape)\n",
    "test = fp.transform(test); print(test.shape)\n",
    "\n",
    "y = y - 1 #fix for zero bound array\n",
    "\n",
    "denom = 0\n",
    "fold = 1 #Change to 5, 1 for Kaggle Limits\n",
    "for i in range(fold):\n",
    "    params = {\n",
    "        'eta': 0.03333,\n",
    "        'max_depth': 4,\n",
    "        'objective': 'multi:softprob',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'num_class': 9,\n",
    "        'seed': i,\n",
    "        'silent': True\n",
    "    }\n",
    "    x1, x2, y1, y2 = model_selection.train_test_split(train, y, test_size=0.18, random_state=i)\n",
    "    watchlist = [(xgb.DMatrix(x1, y1), 'train'), (xgb.DMatrix(x2, y2), 'valid')]\n",
    "    model = xgb.train(params, xgb.DMatrix(x1, y1), 1000,  watchlist, verbose_eval=50, early_stopping_rounds=100)\n",
    "    score1 = metrics.log_loss(y2, model.predict(xgb.DMatrix(x2), ntree_limit=model.best_ntree_limit), labels = list(range(9)))\n",
    "    print(score1)\n",
    "    #if score < 0.9:\n",
    "    if denom != 0:\n",
    "        pred = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit+80)\n",
    "        preds += pred\n",
    "    else:\n",
    "        pred = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit+80)\n",
    "        preds = pred.copy()\n",
    "    denom += 1\n",
    "    submission = pd.DataFrame(pred, columns=['class'+str(c+1) for c in range(9)])\n",
    "    submission['ID'] = pid\n",
    "    submission.to_csv('submission_xgb_fold_'  + str(i) + '.csv', index=False)\n",
    "preds /= denom\n",
    "submission = pd.DataFrame(preds, columns=['class'+str(c+1) for c in range(9)])\n",
    "submission['ID'] = pid\n",
    "submission.to_csv('submission_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7.0, 7.0)\n",
    "xgb.plot_importance(booster=model); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
